# Emmit Nova Sunya 1.2T Configuration
# Total Params: 1.2T | Active Params: ~12B
# Trained on 80T Tokens | 32,768 H100s

model:
  name: "emmit-nova-sunya-1.2t"
  
  # Core Transformer
  hidden_size: 7168
  num_layers: 80
  num_attention_heads: 56
  num_key_value_heads: 8  # GQA
  ffn_hidden_size: 19456  # ~2.7 * hidden_size
  vocab_size: 128000      # Large multilingual vocab
  max_position_embeddings: 32768
  rope_theta: 1000000.0   # Long context
  
  # Mixture of Experts
  num_experts: 32
  num_experts_per_token: 2
  expert_capacity_factor: 1.0
  router_aux_loss_coef: 0.001
  router_z_loss_coef: 0.0001
  
  # Vision
  vision_enabled: true
  vision_hidden_size: 1152 # SigLIP-Large
  vision_num_layers: 27
  vision_num_heads: 16
  image_size: 336
  patch_size: 14

training:
  batch_size: 1
  gradient_accumulation_steps: 128 # Large global batch
  max_steps: 1000000
  learning_rate: 1.5e-4
  min_learning_rate: 1.5e-5
  warmup_steps: 2000
  weight_decay: 0.1
  mixed_precision: "bf16"
  gradient_checkpointing: true
  max_seq_len: 32768

distributed:
  distributed_backend: "nccl"
  sharding_strategy: "FULL_SHARD"
  cpu_offload: false
  # Parallelism strategy for 32k H100s
  tensor_parallel_size: 8
  pipeline_parallel_size: 8
  expert_parallel_size: 64
  data_parallel_size: 64
