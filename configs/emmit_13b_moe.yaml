# Emmit-13B-MoE: Full production model
# ~62B total parameters, ~13B active per token

model:
  name: "emmit-13b-moe"
  hidden_size: 5120
  num_layers: 40
  num_attention_heads: 40
  num_key_value_heads: 5        # GQA: 8x compression
  ffn_hidden_size: 13824        # ~2.7x hidden_size (SwiGLU)
  max_position_embeddings: 131072  # 128K context
  rope_theta: 10000.0
  vocab_size: 128000

  # MoE
  num_experts: 8
  num_experts_per_token: 2      # Top-2 routing
  expert_capacity_factor: 1.25
  router_aux_loss_coef: 0.01
  router_z_loss_coef: 0.001

  # Vision
  vision_enabled: true
  vision_hidden_size: 1024
  vision_num_layers: 24
  image_size: 336
  patch_size: 14

  # Regularization
  dropout: 0.0
  attention_dropout: 0.0

training:
  batch_size: 4
  gradient_accumulation_steps: 16  # Effective batch: 512 (across 8 GPUs)
  max_steps: 1000000
  learning_rate: 1.5e-4
  min_learning_rate: 1.5e-5
  weight_decay: 0.1
  warmup_steps: 2000
  lr_scheduler: "cosine"
  max_grad_norm: 1.0

  # Precision
  mixed_precision: "bf16"
  gradient_checkpointing: true

  # Logging & saving
  eval_steps: 500
  save_steps: 2000
  logging_steps: 10
  max_checkpoints: 5

  # Data
  max_seq_len: 4096             # Start at 4K, scale to 128K later
  pack_sequences: true

distributed:
  backend: "nccl"
  sharding_strategy: "FULL_SHARD"
  cpu_offload: false
  expert_parallel_size: 4
