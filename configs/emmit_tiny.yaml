# Emmit-Tiny: Scaled-down model for single-GPU debugging & validation
# ~25M parameters (active), fast iteration

model:
  name: "emmit-tiny"
  hidden_size: 512
  num_layers: 8
  num_attention_heads: 8
  num_key_value_heads: 2        # GQA: 4x compression
  ffn_hidden_size: 1376         # ~2.7x hidden_size (SwiGLU)
  max_position_embeddings: 4096
  rope_theta: 10000.0
  vocab_size: 32000

  # MoE
  num_experts: 4
  num_experts_per_token: 2      # Top-2 routing
  expert_capacity_factor: 1.25
  router_aux_loss_coef: 0.01
  router_z_loss_coef: 0.001

  # Vision (disabled for tiny)
  vision_enabled: false
  vision_hidden_size: 512
  vision_num_layers: 6
  image_size: 336
  patch_size: 14

  # Regularization
  dropout: 0.0
  attention_dropout: 0.0

training:
  batch_size: 2
  gradient_accumulation_steps: 8   # Effective batch: 16
  max_steps: 10000
  learning_rate: 3.0e-4
  min_learning_rate: 3.0e-5
  weight_decay: 0.1
  warmup_steps: 1000
  lr_scheduler: "cosine"
  max_grad_norm: 1.0

  # Precision
  mixed_precision: "bf16"
  gradient_checkpointing: true

  # Logging & saving
  eval_steps: 100
  save_steps: 500
  logging_steps: 10
  max_checkpoints: 3

  # Data
  max_seq_len: 2048
  pack_sequences: true
